```{r quickstart-setup, include=FALSE, purl=FALSE}
knitr::opts_knit$set(unnamed.chunk.label = "quick-")
```

# How to get started (quickly)

If you already work with models in the form of Equation \ref{eqn:marss}, you can immediately fit your model with the **MARSS** package.  Install the **MARSS** package and then type `library(MARSS)` at the command line to load the package.  Look at the Quick Start Guide and then skim through Chapter \ref{chap:Examples} to, hopefully, find an example similar to your application.  Appendix \ref{chap:modelspec} also has many examples of how to specify different forms for your parameter matrices. 

## Getting your data in right format

Your data need to be a matrix (not dataframe nor a ts object) with time across the columns ($n \times T$ matrix).  The **MARSS** functions assume discrete time steps and you will need a column for each time step.  Replace any missing time steps with NA.  Write your model down on paper and identify which parameters correspond to which parameter matrices in Equation \ref{eqn:marss}.  Call the `MARSS()` function (Chapter \ref{chap:MARSS}) using your data and using the `model` argument to specify the structure of each parameter. 

A R **ts** object (time series object) stores information about the time steps of the data and often seasonal information (the quarter or month).  `MARSS()` needs this information in matrix form.  If you have your data in **ts** form, then you are probably using year and season (quarter, month) as covariates to estimate trend and seasonality.  Here is how to get your **ts** into the form that `MARSS()` wants.

Here is how to get your ts into the form that `MARSS()` needs.

### Univariate example

This converts a univariate ts object with year and quarter into a matrix with a row for the response (here called `Temp`), year, and quarter.  

```
z = ts(rnorm(10), frequency = 4, start = c(1959, 2))
dat = data.frame(Yr = floor(time(z) + .Machine$double.eps), 
      Qtr = cycle(z), Temp=z)
dat = t(dat)
```
When you call `MARSS()`, `dat["Temp",]` is the data. `dat[c("Yr","Qtr"),]` are your covariates.  

### Multivariate example

In this example, we have two temperature readings and a salinity reading. The data are monthly.

```
z <- ts(matrix(rnorm(300), 100, 3), start = c(1961, 1),
     frequency = 12, names=c("Temp1","Temp2","Sal"))
dat = data.frame(Yr = floor(time(z) + .Machine$double.eps), 
     Month = cycle(z), z)
```

When you call `MARSS()`, `dat[c("Temp1","Temp2"),]` are the data and `dat[c("Yr","Month","Sal"),]` are your covariates.  

See the chapters that discuss seasonality for examples of how to model seasonality.  The brute force method of treating month or quarter as a factor requires estimation of more parameters than necessary in many cases.

## Important notes about the algorithms

Specification of a properly constrained model with a unique solution is the responsibility of the user because `MARSS()` has no way to tell if you have specified an insufficiently constrained model---with correspondingly an infinite number of solutions. 
How do you know if the model is properly constrained?  If you are using a MARSS model form that is widely used, then you can probably assume that it is properly constrained. If you go to papers where someone developed the model or method, the issue of constraints necessary to ensure "identifiability" will likely be addressed if it is an issue.  Are you fitting novel MARSS models? Then you will need to do some study on identifiability in this class of models using textbooks (Appendix \ref{chap:SSreferences}).  Often textbooks do not address identifiability explicitly.  Rather it is addressed implicitly by only showing a model constructed in such a way that it is identifiable.  In our work, if we suspect identification problems, we will often first do a Bayesian analysis with flat priors and look for oddities in the posteriors, such as ridges, plateaus or bimodality.
 
All the EM code in the **MARSS** package is currently in native \,\textbf{R}.  Thus the model fitting is slow.  The classic Kalman filter/smoother algorithm, as shown in \citet[p. 331-335]{ShumwayStoffer2006}, is based on the original smoother presented in \citet{Rauch1963}. This Kalman filter is provided in function `MARSSkfss()`, but the default Kalman filter and smoother used in the **MARSS** package is based on the algorithm in \citet{KohnAnsley1989} and papers by Koopman et al. This Kalman filter and smoother is provided in the **KFAS** package (Helske 2012).  Table 2 in \citet{Koopman1993} indicates that the classic algorithm is 40-100 times slower than the algorithm given in \citet{KohnAnsley1989}, \citet{Koopman1993}, and \citet{Koopmanetal1998}. The **MARSS** package function `MARSSkfas()` provides a translator between the model objects in **MARSS** and those in **KFAS** so that the **KFAS** functions can be used.  `MARSSkfas()` also includes a lag-one covariance smoother algorithm as this is not output by the **KFAS** functions, and it provides proper formulation of the priors so that one can use the **KFAS** functions when the prior on the states is set at $t=0$ instead of $t=1$. Simply off-setting your data to start at t=2 and sending that value to $t_{init}=1$ in the **KFAS** Kalman filter would not be mathematically correct!

EM algorithms will quickly get in the vicinity of the maximum likelihood, but the final approach to the maximum is generally slow relative to quasi-Newton methods.  On the flip side, EM algorithms are quite robust to initial conditions choices and can be extremely fast at getting close to the MLE values for high-dimensional models.  The **MARSS** package also allows one to use the BFGS method to fit MARSS models, thus one can use an EM algorithm to ``get close'' and then the BFGS algorithm to polish off the estimate. Restricted maximum-likelihood algorithms\index{estimation!REML} are also available for AR(1) state-space models, both univariate \citep{Staplesetal2004} and multivariate \citep{HinrichsenHolmes2009}.  REML can give parameter estimates with lower variance than plain maximum-likelihood algorithms.  However, the algorithms for REML when there are missing values are not currently available (although that will probably change in the near future).  Another maximum-likelihood method is data-cloning which adapts MCMC algorithms used in Bayesian analysis for maximum-likelihood estimation \citep{Leleetal2007}.  

Missing values\index{missing values} are seamlessly accommodated with the **MARSS** package.  Simply specify missing data with NAs.  The likelihood computations are exact and will deal appropriately with missing values.  However, no innovations, referring to the non-parametric bootstrap developed by Stoffer and Wall (1991), bootstrapping can be done if there are missing values.  Instead parametric bootstrapping must be used.

You should be aware that maximum-likelihood estimates of variance in MARSS models are fundamentally biased, regardless of the algorithm used.  This bias is more severe when one or the other of $\mathbf{R}$ or $\mathbf{Q}$ is very small, and the bias does not go to zero as sample size goes to infinity.  The bias arises because variance is constrained to be positive.  Thus if $\mathbf{R}$ or $\mathbf{Q}$ is essentially zero, the mean estimate will not be zero and thus the estimate  will be biased high while the corresponding bias of the other variance will be biased low.  You can generate unbiased variance estimates using a bootstrap estimate of the bias.  The function `MARSSparamCIs()`\index{functions!MARSSparamCIs} will do this.  However be aware that adding an  estimated bias to a parameter estimate will lead to an increase in the variance of your parameter estimate.  The amount of variance added will depend on sample size.

You should also be aware that mis-specification of the prior on the initial states ($\boldsymbol{\pi}$ and $\boldsymbol{\Lambda}$) can have catastrophic effects on your parameter estimates if your prior conflicts with the distribution of the initial states implied by the MARSS model\index{prior!troubleshooting}.  These effects can be very difficult to detect because the model will appear to be well-fitted.  Unless you have a good idea of what the parameters should be, you might not realize that your prior conflicts.  

The most common problems we have found with priors on $\mathbf{x}_0$ are the following.  Problem 1) The correlation structure in $\boldsymbol{\Lambda}$ (whether the prior is diffuse or not) does not match the correlation structure in $\mathbf{x}_0$ implied by your model.  For example, you specify a diagonal $\boldsymbol{\Lambda}$ (independent states), but the implied distribution has correlations. Problem 2) The correlation structure in $\boldsymbol{\Lambda}$ does not match the structure in $\mathbf{x}_0$ implied by constraints you placed on $\boldsymbol{\pi}$.  For example, you specify that all values in $\boldsymbol{\pi}$ are shared, yet you specify that $\boldsymbol{\Lambda}$ is diagonal (independent).  

Unfortunately, using a diffuse prior does not help with these two problems because  the diffuse prior still has a correlation structure and can still conflict with the implied correlation in $\mathbf{x}_0$.  One way to get around these problems is to set $\boldsymbol{\Lambda}$=0 (a $m \times m$ matrix of zeros) and estimate $\boldsymbol{\pi} \equiv \mathbf{x}_0$ only.  Now $\boldsymbol{\pi}$ is a fixed but unknown (estimated) parameter, not the mean of a distribution.  In this case, $\boldsymbol{\Lambda}$ does not exist in your model and there is no conflict with the model.  
Be aware however that estimating $\boldsymbol{\pi}$ as a parameter is not always robust. If you specify that $\boldsymbol{\Lambda}$=0 and specify that $\boldsymbol{\pi}$ corresponds to $\mathbf{x}_0$, but your model ``explodes'' when run backwards in time, you cannot estimate $\boldsymbol{\pi}$ because you cannot get a good estimate of $\mathbf{x}_0$.  Sometimes this can be avoided by specifying that $\boldsymbol{\pi}$ corresponds to $\mathbf{x}_1$ so that it can be constrained by the data $\mathbf{y}_1$. 

In summary, if the implied correlation structure of your initial states is independent (diagonal variance-covariance matrix), you should generally be ok with a diagonal and high variance prior or with treating the initial states as parameters (with $\boldsymbol{\Lambda}=0$).  But if your initial states have an implied correlation structure that is not independent, then proceed with caution. `With caution' means that you should assume you have problems and test how your model fits with simulated data.

## State-space form of ARMA(p,q) models

There is a large class of models in the statistical finance literature that have the form
\begin{equation*}
\begin{gathered}
\mathbf{x}_{t+1} = \mathbf{B}\mathbf{x}_t + \boldsymbol{\Gamma}\boldsymbol{\eta}_t\\
\mathbf{y}_t = \mathbf{Z}\mathbf{x}_t  + \boldsymbol{\eta}_t\\
\end{gathered}
\end{equation*}
For example, ARMA(p,q) models can be written in this form.  The MARSS model framework in this package will not allow you to write models in that form.  You can put the $\boldsymbol{\eta}_t$ into the $\mathbf{x}_t$ vector and set $\mathbf{R}=0$ to make models of this form using the MARSS form, but the EM algorithm in the **MARSS** package won't let you estimate parameters because the parameters will drop out of the full likelihood being maximized in the algorithm.  You can try using BFGS by passing in the `method` argument to the `MARSS()` call.

## Troubleshooting

\index{troubleshooting}Numerical errors due to ill-conditioned matrices are not uncommon when fitting MARSS models\index{errors!ill-conditioned}\index{troubleshooting!ill-conditioning}\index{likelihood!troubleshooting}. The Kalman and EM algorithms need inverses of matrices. If those matrices become ill-conditioned, for example all elements are close to the same value, then the algorithm becomes unstable.  Warning messages will be printed if the algorithms are becoming unstable\index{troubleshooting!numerical instability} and you can set `control$trace=1`, to see details of where the algorithm is becoming unstable.  Whenever possible, you should avoid using shared $\boldsymbol{\pi}$ values in your model\footnote{An example of a $\boldsymbol{\pi}$ with shared values is $\boldsymbol{\pi}=\bigl[\begin{smallmatrix} a\\a\\a \end{smallmatrix} \bigr]$.}.  The way our algorithm deals with $\boldsymbol{\Lambda}$ tends to make this case unstable, especially if $\mathbf{R}$ is not diagonal.  In general, estimation of a non-diagonal $\mathbf{R}$ is more difficult, more prone to ill-conditioning, and more data-hungry.

You may also see non-convergence warnings, especially if your MLE model turns out to be degenerate\index{errors!degenerate}\index{troubleshooting!degenerate}\index{troubleshooting!non-convergence}.  This means that one of the elements on the diagonal of your $\mathbf{Q}$ or $\mathbf{R}$ matrix are going to zero (are degenerate).  It will take the EM algorithm forever to get to zero.  BFGS will have the same problem, although it will often get a bit closer to the degenerate solution.  If you are using `method="kem"`, `MARSS()` will warn you if it looks like the solution is degenerate. If you use `control=list(allow.degen=TRUE)`, the EM algorithm will attempt to set the degenerate variances to zero (instead of trying to get to zero using an infinite number of iterations).  However, if one of the variances is going to zero, first think about why this is happening.  This is typically caused by one of three problems:  1) you made a mistake in inputting your data, e.g. used -99 as the missing value in your data but did not replace these with NAs before passing to `MARSS()`, 2) your data are not sufficient to estimate multiple variances or 3) your data are inconsistent with the model you are trying fit.

The algorithms in the **MARSS** package are designed for cases where the $\mathbf{Q}$ and $\mathbf{R}$ diagonals are all non-minuscule.  For example, the EM update equation for $\mathbf{u}$ will grind to a halt (not update $\mathbf{u}$) if $\mathbf{Q}$ is tiny (like 1E-7).  Conversely, the BFGS equations are likely to miss the maximum-likelihood when $\mathbf{R}$ is tiny because then the likelihood surface becomes hyper-sensitive to $\boldsymbol{\pi}$.   The solution is to use the degenerate likelihood function for the likelihood calculation and the EM update equations.  `MARSS()` will implement this automatically when $\mathbf{Q}$ or $\mathbf{R}$ diagonal elements are set to zero and will try setting $\mathbf{Q}$ and $\mathbf{R}$ terms to zero automatically if `control$allow.degen=TRUE`.  

One odd case can occur when $\mathbf{R}$ goes to zero (a matrix of zeros), but you are estimating $\boldsymbol{\pi}$.  If `model$tinitx=1`, then $\boldsymbol{\pi}=\mathbf{x}_1^0$ and $\mathbf{y}_1-\mathbf{Z}\mathbf{x}_1^0$ can go to 0 as well as $\,\text{var}(\mathbf{y}_1-\mathbf{Z}\mathbf{x}_1^0)$ by driving $\mathbf{R}$  to zero. But as this happens, the log-likelihood associated with $\mathbf{y}_1$ will go (correctly) to infinity and thus the log-likelihood goes to infinity.  But if you set $\mathbf{R}=0$, the log-likelihood will be finite.  The reason is that $\mathbf{R} \approx 0$ and $\mathbf{R}=0$ specify different likelihoods associated with $\mathbf{y}_1-\mathbf{Z}\mathbf{x}_1^0$.  With $\mathbf{R}=0$, $\mathbf{y}_1-\mathbf{Z}\mathbf{x}_1^0$ does not have a distribution; it is just a fixed value.  So there is no likelihood to go to infinity.  If some elements of the diagonal of $\mathbf{R}$ are going to zero, you should be suspect of the parameter estimates.  Sometimes the structure of your data, e.g. one data value followed by a long string of missing values, is causing an odd spike in the likelihood at  $\mathbf{R} \approx 0$.  Try manually setting $\mathbf{R}$ equal to zero to get the correct log-likelihood\footnote{The likelihood returned when $\mathbf{R} \approx 0$ is not incorrect.  It is just not the likelihood that you probably want.  You want the likelihood where the $\mathbf{R}$ term is dropped because it is zero.}.  

\section{Other related packages}
Packages that will do Kalman filtering and smoothing are many, but packages that estimate the parameters in a MARSS model, especially constrained MARSS models, are much less common.  The following are those with which we are familiar, however there are certainly more packages for estimating MARSS models in engineering and economics of which we are unfamiliar.  The **MARSS** package is unusual in that it uses an EM algorithm for maximizing the likelihood as opposed to a Newton-esque method (e.g. BFGS). The package is also unusual in that it allows you to specify the initial conditions at $t=0$ or $t=1$, allows degenerate models (with some of the diagonal elements of $\mathbf{R}$ or $\mathbf{Q}$ equal to zero). Lastly, model specification in the **MARSS** package  has a one-to-one relationship between the model list in `MARSS()` and the model as you would write it on paper as a matrix equation.  This makes the learning curve a bit less steep.  However, the **MARSS** package has not been optimized for speed and probably will be really slow if you have time-series data with a lot of time points.

\begin{description}
	\item[atsar] \href{https://nwfsc-timeseries.github.io/atsar/}{atsar} is an \,\textbf{R} package we wrote for fitting MARSS models using STAN.  It allows fast and flexible fitting of MARSS models in a Bayesian framework.  Our book from our time-series class has example applications \href{https://nwfsc-timeseries.github.io/atsa-labs/}{Applied Time-Series Analysis for Fisheries and Environmental Sciences}.
  \item[stats] The **stats** package (part of base R) has functions for fitting univariate structural time series models (MARSS models with a univariate $y$).  Read the help file at `?StructTS`. The Kalman filter and smoother functions are described here: `?KalmanLike`.
	\item[DLM] \href{https://cran.r-project.org/package=dlm}{DLM} is an \,\textbf{R} package for fitting MARSS models.  Our impression is that it is mainly Bayesian focused but it does allow MLE estimation via the `optim()` function.  It has a book, Dynamic Linear Models with \,\textbf{R}  by Petris et al., which has many examples of how to write MARSS models for different applications.
	\item[sspir] \href{https://cran.r-project.org/package=sspir}{sspir} an \,\textbf{R} package for fitting ARSS (univariate) models with Gaussian, Poisson and binomial error distributions.  
	\item[dse] \href{https://cran.r-project.org/package=dse}{dse} (Dynamic Systems Estimation) is an \,\textbf{R} package for multivariate Gaussian state-space models with a focus on ARMA models.
	\item[SsfPack] \href{http://www.ssfpack.com/}{SsfPack} is a package for Ox/Splus that fits constrained multivariate Gaussian state-space models using mainly (it seems) the BFGS algorithm but the newer versions support other types of maximization.  SsfPack is very flexible and written in C to be fast.  It has been used extensively on statistical finance problems and is optimized for dealing with large (financial) data sets.  It is used and documented in Time Series Analysis by State Space Methods by Durbin and Koopman, An Introduction to State Space Time Series Analysis by Commandeur and Koopman, and Statistical Algorithms for Models in State Space Form: SsfPack 3.0, by Koopman, Shephard, and Doornik.
	\item[Brodgar] The Brodgar software was developed by Alain Zuur to do (among many other things) dynamic factor analysis, which involves a special type of MARSS model.  The methods and many example analyses are given in Analyzing Ecological Data by Zuur, Ieno and Smith.  This is the one package that we are aware of that also uses an EM algorithm for parameter estimation.
	\item[eViews] eViews is a commercial economics software that will estimate at least some types of MARSS models.
	\item[KFAS] The \href{https://cran.r-project.org/package=KFAS}{KFAS} \,\textbf{R} package provides a fast Kalman filter and smoother.  Examples in the package show how to estimate MARSS models using the **KFAS** functions and \,\textbf{R}'s `optim()` function.   The **MARSS** package uses the filter and smoother functions from the **KFAS** package.
		\item[S+FinMetrics] \href{http://faculty.washington.edu/ezivot/MFTS2ndEditionFinMetrics.htm}{S+FinMetrics} is a S-plus module for fitting MAR models, which are called vector autoregressive (VAR) models in the economics and finance literature.  It has some support for state-space VAR models, though we haven't used it so are not sure which parameters it allows you to estimate.  It was developed by Andrew Bruce, Doug Martin, Jiahui Wang, and Eric Zivot, and it has a book associated with it: Modeling Financial Time Series with S-plus by Eric Zivot and Jiahui Wang.
		\item[kftrack] The \href{https://github.com/positioning/kalmanfilter/wiki}{kftrack} \,\textbf{R} package provides a suite of functions specialized for fitting MARSS models to animal tracking data.
\end{description}
