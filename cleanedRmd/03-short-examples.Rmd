```{r short-setup, include=FALSE, purl=FALSE}
knitr::opts_knit$set(unnamed.chunk.label = "short-")
```

# Short Examples

In this chapter, we work through a series of short examples to illustrate the MARSS package functions.  This chapter is oriented towards those who are already somewhat familiar with MARSS models and want to get started quickly.  We provide little explanatory text.  Those unfamiliar with MARSS models might want to start with the application chapters. 

In these examples, we will use the default `form="marxss"` argument for a `MARSS()` call.  This specifies a MARSS model of the form:
\begin{subequations}(\#eq:marss.qe)
\begin{gather}
\mathbf{x}_t = \mathbf{B}_t\mathbf{x}_{t-1} + \mathbf{u}_t + \mathbf{C}_t\mathbf{c}_t + \mathbf{G}_t\mathbf{w}_t, \text{ where } \mathbf{w}_t \sim \,\text{MVN}(0,\mathbf{Q}_t)\\
\mathbf{y}_t = \mathbf{Z}_t\mathbf{x}_t + \mathbf{a}_t + \mathbf{D}_t\mathbf{d}_t + \mathbf{H}_t\mathbf{v}_t, \text{ where } \mathbf{v}_t \sim \,\text{MVN}(0,\mathbf{R}_t)\\
\mathbf{x}_1 \sim \,\text{MVN}(\boldsymbol{\pi},\boldsymbol{\Lambda}) \text{ or } \mathbf{x}_0 \sim \,\text{MVN}(\boldsymbol{\pi},\boldsymbol{\Lambda})
\end{gather}
\end{subequations} 
The $\mathbf{c}$ and $\mathbf{d}$ are inputs (not estimated).  In the examples here, we leave off $\mathbf{c}$ and $\mathbf{d}$.  We address including inputs only briefly at the end of the chapter.  See Chapter \@ref(chap:covariates) for extended examples of including covariates as inputs in a MARSS model.  We will also not use $\mathbf{G}_t$ or $\mathbf{H}_t$ in this chapter. 

## Fixed and estimated elements in parameter matrices
Suppose one has a MARSS model (Equation \@ref(eq:marss.qe)) with the following structure:
\begin{gather*}
\begin{bmatrix}x_{1,t}\\x_{2,t}\end{bmatrix}
= \begin{bmatrix}b_1&0.1\\b_2&2\end{bmatrix}
\begin{bmatrix}x_{1,t-1}\\x_{2,t-1}\end{bmatrix}
+ \begin{bmatrix}u\\u\end{bmatrix}
+ \begin{bmatrix}w_{1,t}\\w_{2,t}\end{bmatrix},
 \textrm{ } \mathbf{w}_t \sim \,\text{MVN}\begin{pmatrix}\begin{bmatrix}0\\0\end{bmatrix},\begin{bmatrix}q_1&q_3\\q_3&q_2\end{bmatrix} \end{pmatrix}  \\
\\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\end{bmatrix}
= \begin{bmatrix}z_1&0\\z_2&z_2\\0&3\end{bmatrix}
\begin{bmatrix}x_{1,t}\\x_{2,t}\end{bmatrix}
+ \begin{bmatrix}0\\0\\0\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\end{bmatrix},
 \textrm{ } \mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix}\begin{bmatrix}0\\0\\0\end{bmatrix},\begin{bmatrix}r&0&0\\0&r&0\\0&0&1\end{bmatrix} \end{pmatrix}  \\
\mathbf{x}_0 \sim \,\text{MVN}\begin{pmatrix}\begin{bmatrix}\pi_1\\ \pi_2\end{bmatrix},\begin{bmatrix}1&0\\0&1\end{bmatrix} \end{pmatrix} 
\end{gather*}
Notice how this model mixes fixed values, estimated values and shared values.

In MARSS, model structure is specified using a list with the names, `Z`, `A`, `R`, `B`, `U`, `Q`, `x0` and `V0`.  Each element is matrix (class matrix) with the same dimensions as the matrix of the same name in the MARSS model.  MARSS distinguishes between the estimated and fixed values in a matrix by using a list matrix in which you can have numeric and character elements.  Numeric elements are fixed; character elements are names of things to be estimated.  The model above would be specified as:
```{r model-gen-spec, eval=TRUE}
Z=matrix(list("z1","z2",0,0,"z2",3),3,2)
A=matrix(0,3,1)
R=matrix(list(0),3,3); diag(R)=c("r","r",1)
B=matrix(list("b1",0.1,"b2",2),2,2)
U=matrix(c("u","u"),2,1)
Q=matrix(c("q1","q3","q3","q2"),2,2)
x0=matrix(c("pi1","pi2"),2,1)
V0=diag(1,2)
model.gen=list(Z=Z,A=A,R=R,B=B,U=U,Q=Q,x0=x0,V0=V0,tinitx=0)
```
Notice that there is a one-to-one correspondence between the model list in \,\textbf{R} and the model on paper.  Fitting the model is then just a matter of passing the data and model list to the MARSS function:
```{r model-general, eval=FALSE}
kemfit = MARSS(dat, model=model.gen)
```

\index{troubleshooting!sensitivity to x0 prior}If you work often with MARSS models then you will probably know whether prior sensitivity is a problem for your types of MARSS applications.  If so, note that the MARSS package is unusual in that it allows you to set $\boldsymbol{\Lambda}=0$ and treat $\mathbf{x}_0$ as an unknown estimated parameter\index{prior}.  This eliminates the prior and thus the prior sensitivity problems---at the cost of adding $m$ parameters.  Depending on your application, you may need to set the initial conditions at $t=1$ instead of the default of $t=0$.  If you are unsure, look in the index and read all the sections that talk about troubleshooting priors.

## Different numbers of state processes
Here we show a series of short examples using a dataset on Washington harbor seals (`?harborSealWA`), which has five observation time series.   The dataset is a little unusual in that it has four missing years from years 2 to 5.  This causes some interesting issues with prior specification.  Before starting the harbor seal examples, we set up the data, making time go across the columns and removing the year column:
```{r enterdata, keep.source=T}
dat = t(harborSealWA)
dat = dat[2:nrow(dat),] #remove the year row
```

### One hidden state process for each observation time series
This is the default model for the `MARSS()` function.  In this case, $n=m$, the observation errors are i.i.d. and the process errors are independent and have different variances.  The elements in $\mathbf{u}$ are all different (meaning, they are not forced to be the same).  Mathematically, the MARSS model being fit is:
\begin{gather*}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
= \begin{bmatrix}1&0&0&0&0\\0&1&0&0&0\\0&0&1&0&0\\0&0&0&1&0\\0&0&0&0&1\end{bmatrix}
\begin{bmatrix}x_{1,t-1}\\x_{2,t-1}\\x_{3,t-1}\\x_{4,t-1}\\x_{5,t-1}\end{bmatrix}
+ \begin{bmatrix}u_1\\u_2\\u_3\\u_4\\u_5\end{bmatrix}
+ \begin{bmatrix}w_{1,t}\\w_{2,t}\\w_{3,t}\\w_{4,t}\\w_{5,t}\end{bmatrix},
 \textrm{ } \mathbf{w}_t \sim \,\text{MVN}\begin{pmatrix}\begin{bmatrix}0\\0\\0\\0\\0\end{bmatrix},\begin{bmatrix}q_1&0&0&0&0\\0&q_2&0&0&0\\0&0&q_3&0&0\\0&0&0&q_4&0\\0&0&0&0&q_5\end{bmatrix} \end{pmatrix}  \\
\\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\\y_{4,t}\\y_{5,t}\end{bmatrix}
= \begin{bmatrix}1&0&0&0&0\\0&1&0&0&0\\0&0&1&0&0\\0&0&0&1&0\\0&0&0&0&1\end{bmatrix}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
+ \begin{bmatrix}0\\0\\0\\0\\0\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\\ v_{4,t}\\ v_{5,t}\end{bmatrix},
 \textrm{ } \mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix}\begin{bmatrix}0\\0\\0\\0\\0\end{bmatrix},\begin{bmatrix}r&0&0&0&0\\0&r&0&0&0\\0&0&r&0&0\\0&0&0&r&0\\0&0&0&0&r\end{bmatrix} \end{pmatrix}  \\
\end{gather*}
This is the default model, so you can fit it by simply passing `dat` to `MARSS()`\index{functions!MARSS}.  
```{r model-default}
kemfit = MARSS(dat)
```
The output warns you that the convergence tolerance is high.  You can set it lower by passing in `control=list(conv.test.slope.tol=0.1)`.  `MARSS()` is automatically creating parameter names since you did not tell it the names. To see exactly where each parameter element appears in its parameter matrix, type `summary(kemfit$model)`.
 
 Though it is not necessary to specify the model for this example since it is the default, here is how you could do so using matrices: 
```{r eval=FALSE}
B=Z=diag(1,5)
U=matrix(c("u1","u2","u3","u4","u5"),5,1)
x0=A=matrix(0,5,1)
R=Q=matrix(list(0),5,5)
diag(R)="r"
diag(Q)=c("q1","q2","q3","q4","q5")
```
Notice that when a matrix has both fixed and estimated elements (like $\mathbf{R}$ and $\mathbf{Q}$), a list matrix is used to allow you to specify the fixed elements as numeric and to give the estimated elements character names.
```{r model-default-time, echo=FALSE, results='hide'}
kemfit.time = system.time(MARSS(dat,silent=TRUE))
```

The default MLE method is the EM algorithm (`method="kem"`).  You can also use a quasi-Newton method\index{estimation!quasi-Newton} (BFGS) 
by setting `method="BFGS"`\index{estimation!BFGS}.  
```{r model-bfgs}
kemfit.bfgs = MARSS(dat, method="BFGS")
```
```{r model-bfgs-time, echo=FALSE, results='hide'}
bfgsfit.time = system.time(MARSS(dat,silent=TRUE,method="BFGS"))
```
Using the default EM convergence criteria, the EM algorithm stops at a log-likelihood a little lower than the BFGS algorithm does, but the EM algorithm was faster, `r round(bfgsfit.time[1]/kemfit.time[1],1)` times faster, in this case.
If you wanted to use the EM fit as the initial conditions\index{initial conditions!setting for BFGS}, pass in the `inits` argument using the `$par` element (or `coef(fit,form="marss")`) of the EM fit.  
```{r  model-bfgs2, results='hide' }
kemfit.bfgs2 = MARSS(dat, method="BFGS", inits=kemfit$par)
```
The BFGS algorithm now converges in `r kemfit.bfgs2$numIter` iterations. Output not shown.

\index{troubleshooting!sensitivity to x0 prior}We mentioned that the missing years from year 2 to 4 creates an interesting issue with the prior specification. The default behavior of MARSS is to treat the initial state as at $t=0$ instead of $t=1$. Usually this doesn't make a difference, but for this dataset, if we set the prior at $t=1$, the MLE estimate of $\mathbf{R}$ becomes 0.  If we estimate $\mathbf{x}_1$ as a parameter and let $\mathbf{R}$ go to 0, the likelihood will go to infinity (slowly but surely)\index{prior!troubleshooting}.  This is neither an error nor a pathology, but is probably not what you would like to have happen.   Note that the ``BFGS'' algorithm will not find the maximum in this case; it will stop before $\mathbf{R}$ gets small and the likelihood gets very large.   However, the EM algorithm will climb up the peak.  You can try it by running the following code.  It will report warnings which you can read about in Appendix \@ref(app:warnings).
```{r model-default-strange, eval=FALSE}
kemfit.strange = MARSS(dat, model=list(tinitx=1))
```

### Five correlated hidden state processes
This is the same model except that the five hidden states have correlated process errors.  Mathematically, this is the model:
\begin{gather*}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
= \begin{bmatrix}x_{1,t-1}\\x_{2,t-1}\\x_{3,t-1}\\x_{4,t-1}\\x_{5,t-1}\end{bmatrix}
+ \begin{bmatrix}u_1\\u_2\\u_3\\u_4\\u_5\end{bmatrix}
+ \begin{bmatrix}w_{1,t}\\w_{2,t}\\w_{3,t}\\w_{4,t}\\w_{5,t}\end{bmatrix}, 
 \textrm{ } \mathbf{w}_t \sim \,\text{MVN}\begin{pmatrix}0,
 \begin{bmatrix}q_1&c_{1,2}&c_{1,3}&c_{1,4}&c_{1,5}\\c_{1,2}&q_2&c_{2,3}&c_{2,4}&c_{2,5}\\
 c_{1,3}&c_{2,3}&q_3&c_{3,4}&c_{3,5}\\c_{1,4}&c_{2,4}&c_{3,4}&q_4&c_{4,5}\\
 c_{1,5}&c_{2,5}&c_{3,5}&c_{4,5}&q_5\end{bmatrix} \end{pmatrix} \\
\\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\\y_{4,t}\\y_{5,t}\end{bmatrix}
= \begin{bmatrix}1&0&0&0&0\\0&1&0&0&0\\0&0&1&0&0\\0&0&0&1&0\\0&0&0&0&1\end{bmatrix}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
+ \begin{bmatrix}0\\0\\0\\0\\0\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\\ v_{4,t}\\ v_{5,t}\end{bmatrix}, 
 \textrm{ } \mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix} 0,\begin{bmatrix}r&0&0&0&0\\0&r&0&0&0\\0&0&r&0&0\\0&0&0&r&0\\0&0&0&0&r\end{bmatrix} \end{pmatrix}  \\
\end{gather*}
$\mathbf{B}$ is not shown in the top equation; it is a $m \times m$ identity matrix.  To fit, use `MARSS()`\index{functions!MARSS} with the `model` argument set (output not shown).
```{r model-corr1, results='hide'}
kemfit = MARSS(dat, model=list(Q="unconstrained"))
```
This shows one of the text shortcuts, `"unconstrained"`, which means estimate all elements in the matrix.  This shortcut can be used for all parameter matrices.  

### Five equally correlated hidden state processes
This is the same model except that now there is only one process error variance and one process error covariance.  Mathematically, the model is:
\begin{gather*}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
= \begin{bmatrix}x_{1,t-1}\\x_{2,t-1}\\x_{3,t-1}\\x_{4,t-1}\\x_{5,t-1}\end{bmatrix}
+ \begin{bmatrix}u_1\\u_2\\u_3\\u_4\\u_5\end{bmatrix}
+ \begin{bmatrix}w_{1,t}\\w_{2,t}\\w_{3,t}\\w_{4,t}\\w_{5,t}\end{bmatrix},
 \textrm{ } \mathbf{w}_t \sim \,\text{MVN}\begin{pmatrix} 0,
 \begin{bmatrix}q&c&c&c&c\\c&q&c&c&c\\c&c&q&c&c\\c&c&c&q&c\\c&c&c&c&q\end{bmatrix} \end{pmatrix} \\
\\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\\y_{4,t}\\y_{5,t}\end{bmatrix}
= \begin{bmatrix}1&0&0&0&0\\0&1&0&0&0\\0&0&1&0&0\\0&0&0&1&0\\0&0&0&0&1\end{bmatrix}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
+ \begin{bmatrix}0\\0\\0\\0\\0\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\\ v_{4,t}\\ v_{5,t}\end{bmatrix},
 \textrm{  } \mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix} 0,\begin{bmatrix}r&0&0&0&0\\0&r&0&0&0\\0&0&r&0&0\\0&0&0&r&0\\0&0&0&0&r\end{bmatrix} \end{pmatrix}  \\
\end{gather*}
Again $\mathbf{B}$ is not shown in the top equation; it is a $m \times m$ identity matrix.  To fit, use the following code (output not shown):
```{r model-corr2, include.source=F, keep.source=T, results='hide'}
kemfit = MARSS(dat, model=list(Q="equalvarcov"))
```
The shortcut ``"equalvarcov"` means one value on the diagonal and one on the off-diagonal.  It can be used for all square matrices ($\mathbf{B}$,$\mathbf{Q}$,$\mathbf{R}$, and $\boldsymbol{\Lambda}$).

### Five hidden state processes with a ``north'' and a ``south'' $\mathbf{u}$ and $\mathbf{Q}$ elements
Here we fit a model with five independent hidden states where each observation time series is an independent observation of a different hidden trajectory
but the hidden trajectories 1-3 share their $\mathbf{u}$ and $\mathbf{Q}$ elements, while hidden trajectories 4-5 share theirs.  This is the model:
\begin{gather*}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
= \begin{bmatrix}x_{1,t-1}\\x_{2,t-1}\\x_{3,t-1}\\x_{4,t-1}\\x_{5,t-1}\end{bmatrix}
+ \begin{bmatrix}u_n\\u_n\\u_n\\u_s\\u_s\end{bmatrix}
+ \begin{bmatrix}w_{1,t}\\w_{2,t}\\w_{3,t}\\w_{4,t}\\w_{5,t}\end{bmatrix},  
 \textrm{ } \mathbf{w}_t \sim \,\text{MVN}\begin{pmatrix} 0,\begin{bmatrix}q_n&0&0&0&0\\0&q_n&0&0&0\\0&0&q_n&0&0\\0&0&0&q_s&0\\0&0&0&0&q_s\end{bmatrix} \end{pmatrix}  \\
\\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\\y_{4,t}\\y_{5,t}\end{bmatrix}
= \begin{bmatrix}1&0&0&0&0\\0&1&0&0&0\\0&0&1&0&0\\0&0&0&1&0\\0&0&0&0&1\end{bmatrix}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
+ \begin{bmatrix}0\\0\\0\\0\\0\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\\ v_{4,t}\\ v_{5,t}\end{bmatrix},
 \textrm{ } \mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix} 0,\begin{bmatrix}r&0&0&0&0\\0&r&0&0&0\\0&0&r&0&0\\0&0&0&r&0\\0&0&0&0&r\end{bmatrix} \end{pmatrix}  \\
\end{gather*}
To fit use the following code, we specify the `model` argument for $\mathbf{u}$ and $\mathbf{Q}$ using list matrices. List matrices allow us to combine numeric and character values in a matrix.  MARSS will interpret the numeric values as fixed, and the character values as parameters to be estimated.  Parameters with the same name are constrained to be identical.  
```{r model-u-NS, results='hide'}
regions=list("N","N","N","S","S")
U=matrix(regions,5,1)
Q=matrix(list(0),5,5); diag(Q)=regions
kemfit = MARSS(dat, model=list(U=U, Q=Q))
```
Only $\mathbf{u}$ and $\mathbf{Q}$ need to be specified since the other parameters are at their default values.  

### Fixed observation error variance
Here we fit the same model but with a known observation error variance.  This is the model:
\begin{gather*}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
= \begin{bmatrix}x_{1,t-1}\\x_{2,t-1}\\x_{3,t-1}\\x_{4,t-1}\\x_{5,t-1}\end{bmatrix}
+ \begin{bmatrix}u_n\\u_n\\u_n\\u_s\\u_s\end{bmatrix}
+ \begin{bmatrix}w_{1,t}\\w_{2,t}\\w_{3,t}\\w_{4,t}\\w_{5,t}\end{bmatrix},
 \textrm{ } \mathbf{w}_t \sim \,\text{MVN}\begin{pmatrix} 0,\begin{bmatrix}q_n&0&0&0&0\\0&q_n&0&0&0\\0&0&q_n&0&0\\0&0&0&q_s&0\\0&0&0&0&q_s\end{bmatrix} \end{pmatrix}  \\
\\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\\y_{4,t}\\y_{5,t}\end{bmatrix}
= \begin{bmatrix}1&0&0&0&0\\0&1&0&0&0\\0&0&1&0&0\\0&0&0&1&0\\0&0&0&0&1\end{bmatrix}
\begin{bmatrix}x_{1,t}\\x_{2,t}\\x_{3,t}\\x_{4,t}\\x_{5,t}\end{bmatrix}
+ \begin{bmatrix}0\\0\\0\\0\\0\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\\ v_{4,t}\\ v_{5,t}\end{bmatrix},  \\
\\
\mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix} 0,\begin{bmatrix}0.01&0&0&0&0\\0&0.01&0&0&0\\0&0&0.01&0&0\\0&0&0&0.01&0\\0&0&0&0&0.01\end{bmatrix} \end{pmatrix} \\
\end{gather*}
To fit this model, use the following code (output not shown):\index{functions!MARSS}
```{r model-u-NS-fixR, results='hide'}
regions=list("N","N","N","S","S")
U=matrix(regions,5,1)
Q=matrix(list(0),5,5); diag(Q)=regions
R=diag(0.01,5)
kemfit = MARSS(dat, model=list(U=U, Q=Q, R=R))
```

### One hidden state and five i.i.d. observation time series
Instead of five hidden state trajectories, we specify that there is only one and all the observations are of that one trajectory. Mathematically, the model is:
\begin{gather*}
x_{t}= x_{t-1} + u + w_{t}, \text{ } w_t \sim \,\text{N}(0,q)  \\  \\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\\y_{4,t}\\y_{5,t}\end{bmatrix}
= \begin{bmatrix}1\\1\\1\\1\\1\end{bmatrix}
x_{t} + \begin{bmatrix}0\\a_2\\a_3\\a_4\\a_5\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\\ v_{4,t}\\ v_{5,t}\end{bmatrix},
 \textrm{ } \mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix}0,\begin{bmatrix}r&0&0&0&0\\0&r&0&0&0\\0&0&r&0&0\\0&0&0&r&0\\0&0&0&0&r\end{bmatrix} \end{pmatrix}  \\
\end{gather*}
Note the default model for $\mathbf{R}$ is `"diagonal and equal"`' so we can leave this off when specifying the `model` argument.  To fit, use this code (output not shown):
```{r model-pan1}
Z=factor(c(1,1,1,1,1))
kemfit = MARSS(dat, model=list(Z=Z))
```
You can also pass in $\mathbf{Z}$ exactly as it is in the equation: `Z=matrix(1,5,2)`, but the factor shorthand is handy if you need to assign different observed time series to different underlying state time series (see next examples).   The default $\mathbf{a}$ form is `"scaling"`, which means that the first $\mathbf{y}$ row associated with a given $x$ has $a=0$ and the rest are estimated.

### One hidden state and five independent observation time series with different variances
Mathematically, this model is:
\begin{gather*}
x_{t}= x_{t-1} + u + w_{t}, \text{ } w_t \sim \,\text{N}(0,q)  \\
\\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\\y_{4,t}\\y_{5,t}\end{bmatrix}
= \begin{bmatrix}1\\1\\1\\1\\1\end{bmatrix}
x_{t} + \begin{bmatrix}0\\a_2\\a_3\\a_4\\a_5\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\\ v_{4,t}\\ v_{5,t}\end{bmatrix}, 
 \textrm{ } \mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix} 0,\begin{bmatrix}r_1&0&0&0&0\\0&r_2&0&0&0\\0&0&r_3&0&0\\0&0&0&r_4&0\\0&0&0&0&r_5\end{bmatrix} \end{pmatrix}  \\
\end{gather*}
To fit this model:  \index{functions!MARSS}
```{r model-pan2}
Z=factor(c(1,1,1,1,1))
R="diagonal and unequal"
kemfit = MARSS(dat, model=list(Z=Z, R=R))
```

### Two hidden state processes
Here we fit a model with two hidden states (north and south) where observation time series 1-3 are for the north and 4-5 are for the south.  We make the hidden state processes independent (meaning a diagonal $\mathbf{Q}$ matrix) but with the same process variance.  We make the observation errors i.i.d. (the default) and the $\mathbf{u}$ elements equal.  Mathematically, this is the model:
\begin{gather*}
\begin{bmatrix}x_{n,t}\\x_{s,t}\end{bmatrix}
= \begin{bmatrix}x_{n,t-1}\\x_{s,t-1}\end{bmatrix}
+ \begin{bmatrix}u\\u\end{bmatrix}
+ \begin{bmatrix}w_{n,t}\\w_{s,t}\end{bmatrix},   
 \textrm{ } \mathbf{w}_t \sim \,\text{MVN}\begin{pmatrix} 0,
 \begin{bmatrix}q&0\\0&q\end{bmatrix} \end{pmatrix} \\
\\
\begin{bmatrix}y_{1,t}\\y_{2,t}\\y_{3,t}\\y_{4,t}\\y_{5,t}\end{bmatrix}
= \begin{bmatrix}1&0\\1&0\\1&0\\0&1\\0&1\end{bmatrix}
\begin{bmatrix}x_{n,t}\\x_{s,t}\end{bmatrix}
+ \begin{bmatrix}0\\a_2\\a_3\\0\\a_5\end{bmatrix}
+ \begin{bmatrix}v_{1,t}\\ v_{2,t}\\ v_{3,t}\\ v_{4,t}\\ v_{5,t}\end{bmatrix}, 
 \textrm{ } \mathbf{v}_t \sim \,\text{MVN}\begin{pmatrix}0,\begin{bmatrix}r&0&0&0&0\\0&r&0&0&0\\0&0&r&0&0\\0&0&0&r&0\\0&0&0&0&r\end{bmatrix} \end{pmatrix}  \\
\end{gather*}
To fit the model, use the following code (output not shown):
```{r model-two1, results='hide'}
Z=factor(c("N","N","N","S","S"))
Q="diagonal and equal"
U="equal"
kemfit = MARSS(dat, model=list(Z=Z,Q=Q,U=U))
```
You can also pass in $\mathbf{Z}$ exactly as it is in the equation as a numeric matrix `Z=matrix(c(1,1,1,0,0,0,0,0,1,1),5,2)`; the `factor` notation is a shortcut for making a design matrix (as $\mathbf{Z}$ is in these examples.  `"equal"` is a shortcut meaning all elements in a matrix are constrained to be equal.  It can be used for all column matrices ($\mathbf{a}$, $\mathbf{u}$ and $\boldsymbol{\pi}$).  `"diagonal and equal"` can be used as a shortcut for all square matrices ($\mathbf{B}$,$\mathbf{Q}$,$\mathbf{R}$, and $\boldsymbol{\Lambda}$).

## Time-varying parameters
Time-varying parameters are specified by passing in an array of matrices (list, numeric or character) where the 3rd dimension of the array is time and must be the same value as the 2nd (time) dimension of the data matrix.  No text shortcuts are allowed for time-varying parameters; you need to use the matrix form.

For example, let's say we wanted a different $\mathbf{u}$ for the first half versus second half of the harbor seal time series.  We would pass in an array for $\mathbf{u}$ as follows:
```{r model-time-varying, results='hide'}
U1=matrix("t1",5,1); U2=matrix("t2",5,1)
Ut=array(U2,dim=c(dim(U1),dim(dat)[2]))
TT=dim(dat)[2]
Ut[,,1:floor(TT/2)]=U1
kemfit.tv=MARSS(dat,model=list(U=Ut,Q="diagonal and equal"))
```
You can have some elements in a parameter matrix be time-constant and some be time-varying:
```{r model-time-varying2, results='hide'}
U1=matrix(c(rep("t1",4),"hc"),5,1); U2=matrix(c(rep("t2",4),"hc"),5,1)
Ut=array(U2,dim=c(dim(U1),dim(dat)[2]))
Ut[,,1:floor(TT/2)]=U1
kemfit.tv=MARSS(dat,model=list(U=Ut,Q="diagonal and equal"))
```
Note that how the time-varying model is specified for MARSS is the same as you would write the time-varying model on paper in matrix math form.

## Including inputs (or covariates)
\index{covariates}
In MARSS models with covariates, the covariates are often treated as inputs and appear as either the $\mathbf{c}$ or $\mathbf{d}$ in Equation \@ref(eq:marss.qe), depending on the application. However, more generally, $\mathbf{c}$ and $\mathbf{d}$ are simply inputs that are fully-known (no missing values). $\mathbf{c}_t$ is the $p \times 1$ vector of inputs at time $t$ which affect the states and $\mathbf{d}_t$ is a $q \times 1$ vector of inputs (potentially the same as $\mathbf{c}_t$), which affect the observations.   

$\mathbf{C}_t$ is an $m \times p$ matrix of coefficients relating the effects of $\mathbf{c}_t$ to the $m \times 1$ state vector $\mathbf{x}_t$, and $\mathbf{D}_t$ is an $n \times q$ matrix of coefficients relating the effects of $\mathbf{d}_t$ to the $n \times 1$ observation vector $\mathbf{y}_t$.  The elements of $\mathbf{C}$ and $\mathbf{D}$ can be estimated, and their form is specified much like the other matrices.

With the `MARSS()` function, one can fit a model with inputs by simply passing in `model$c` and/or `model$d` in the MARSS() call as a $p \times T$ or $q \times T$ matrix, respectively.  The form for $\mathbf{C}_t$ and $\mathbf{D}_t$ is similarly specified by passing in `model$C` and/or `model$D`.  If $\mathbf{C}$ and $\mathbf{D}$ are not time-varying, they are passed in as a 2-dimensional matrix.  If they are time-varying, they must be passed in as an 3-dimensional array with the 3rd dimension equal to the number of time steps if they are time-varying.

See Chapter \@ref(chap:covariates) for extended examples of including covariates as inputs in a MARSS model.  Also note that it is not necessary to have your covariates appear in $\mathbf{c}$ and/or $\mathbf{d}$.  That is a common form, however in some MARSS models, covariates will appear in one of the parameter matrices as fixed values.

## Printing and summarizing models and model fits
\index{print}\index{functions!print}
The package includes print functions for marssMODEL objects and marssMLE objects (fitted models).\index{functions!summary}\index{print!marssMODEL}\index{print!marssMLE}
```{r model-print, results='hide'}
print(kemfit)
print(kemfit$model)
```
This will print the basic information on model structure and model fit that you have seen in the previous examples.  The package also includes a summary function for models.
```{r model-summary, results='hide'}
summary(kemfit$model)
```
Output for the summary function is not shown because it is verbose. It prints each matrix with the fixed elements denoted with their values and the free elements denoted by their names.  This is very helpful for confirming exactly what model structure you are fitting to the data.

The print function will also print various other types of output such as a vector of the estimated parameters, the estimated states, the state standard errors, etc. You use the `what` argument in the print call to specify the desired output. \index{print!par}
```{r model-print-par, results='hide'}
print(kemfit, what="par")
```
will print the `par` element of a marssMLE object.  This will only include the estimated elements in a column matrix.  To see the entire parameter matrix with both estimated and fixed values, you can use:
```{r model-print-Q}
print(kemfit, what="Q")
```
Type `?print.MARSS` to see a list of the types of output that can be printed with a `print` call.  If you want to use the output from print instead of printing to the console, then assign the print call to a value:\index{print!states}
```{r model-print-R}
x=print(kemfit, what="states",silent=TRUE)
```

## Tidy output

The `augment`, `tidy` and `glance` functions from the broom package will provide summaries as a data.frame for use in further analyses and for passing to `ggplot()`.  See `?augment.marssMLE` for examples.\index{augment}\index{functions!augment}\index{tidy}\index{functions!tidy}

```{r model-tidy-R}
require(broom)
head(augment(kemfit))
tidy(kemfit)
glance(kemfit)
```

## Confidence intervals on a fitted model
\index{confidence intervals!Hessian approximation}
The function `MARSSparamCIs()`\index{functions!MARSSparamCIs} is used to compute confidence intervals with a default alpha level of 0.05.  The default is to compute approximate confidence intervals using a the Hessian matrix (`method="hessian"`).  Confidence intervals can also be computed  via parametric (`method="parametric"`)   or non-parametric (`method="innovations"`)  bootstrapping.  Note, if you want confidence intervals on variances, then it is unwise to use the Hessian approximation as it is symmetric and variances are constrained to be positive.

### Approximate confidence intervals from a Hessian matrix

The default method for `MARSSparamCIs` computes approximate confidence intervals using an analytically computed Hessian matrix [sec 3.4.5, @Harvey1989].  The call is:
```{r CIs-hessian, include.source=F, keep.source=T}
kem.with.hess.CIs = MARSSparamCIs(kemfit) 
```
See `?MARSShessian` for a discussion of the Hessian calculations.
Use `print` or just type the marssMLE object name to see the confidence intervals:
```{r print-CIs}
print(kem.with.hess.CIs) 
```

### Confidence intervals from a parametric bootstrap
\index{confidence intervals!parametric bootstrap}
Use `method="parametric"` to use a parametric bootstrap to compute confidence intervals and  bias using a parametric bootstrap. 
```{r CIs-pboot, include.source=F, keep.source=T}
kem.w.boot.CIs=MARSSparamCIs(kemfit,method="parametric",nboot=10) 
#nboot should be more like 1000, but set low for example's sake
print(kem.w.boot.CIs)
```

## Vectors of just the estimated parameters
Often it is useful to have a vector of the estimated parameters.  For example, if you are writing a call to `optim`, you will need a 
vector of just the estimated parameters\index{functions!coef}.  You can use the function `coef`:
```{r parvec}
parvec=coef(kemfit, type="vector")
parvec
```

## Kalman filter and smoother output
All the standard Kalman filter and smoother output (along with the lag-one covariance smoother output) is available using the `MARSSkf`\index{functions!MARSSkf}\index{Kalman filter and smoother}\index{lag-1 covariance smoother} function.  Read the help file (`?MARSSkf`) for details and meanings of the names in the output list.
```{r marsskf}
kf=MARSSkf(kemfit)
names(kf)
#if you only need the logLik, this is the fast way to get it
MARSSkf(kemfit, only.logLik=TRUE)
```

## Degenerate variance estimates
If your data are short relative to the number of parameters you are estimating, then you are liable to find that some of the variance elements are degenerate (equal to zero).  Try the following:
```{r like.kem.degen, include.source=F, keep.source=T}
dat.short = dat[1:4,1:10]
kem.degen = MARSS(dat.short,control=list(allow.degen=FALSE))
```
This will print a warning that the maximum number of iterations was reached before convergence of some of the $\mathbf{Q}$ parameters.  It might be that if you just ran a few more iterations the variances will converge.  So first try setting `control$maxit` higher.
```{r like.kem200-degen2, eval=FALSE}
kem.degen2 = MARSS(dat.short, control=list(maxit=1000, 
      allow.degen=FALSE), silent=2)
```
Output not shown, but if you run the code, you will see that some of the $\mathbf{Q}$ terms are still not converging.  MARSS can detect if a variance is going to zero and it will try zero to see if that has a higher likelihood.  Try removing the `allow.degen=FALSE` which was turning off this feature.
```{r like.kem.allow.degen, include.source=F}
kem.short = MARSS(dat.short)
```
So three of the four $\mathbf{Q}$ elements are going to zero.  This often happens when you do not have enough data to estimate both observation and process variance.  

Perhaps we are trying to estimate too many variances.  We can try using only one variance value in $\mathbf{Q}$ and one $u$ value in $\mathbf{u}$:
```{r like.kem200}
kem.small=MARSS(dat.short,model=list(Q="diagonal and equal",
   U="equal"))
```
No, there are simply not enough data to estimate both process and observation variances.  

## Bootstrap parameter estimates
You can easily produce bootstrap parameter estimates from a fitted model using `MARSSboot()`\index{functions!MARSSboot}: 
```{r boot}
boot.params = MARSSboot(kemfit, 
     nboot=20, output="parameters", sim="parametric")$boot.params
```
Use `silent=TRUE` to stop the progress bar from printing. The function will also produce parameter sets generated using a Hessian matrix (`sim="hessian"`) or a non-parametric bootstrap (`sim="innovations"`).


## Data simulation
### Simulated data from a fitted MARSS model
Data can be simulated from  marssMLE object using `MARSSsimulate()`\index{functions!MARSSsimulate}.
```{r sim, include.source=F, keep.source=T}
sim.data=MARSSsimulate(kemfit, nsim=2, tSteps=100)$sim.data
```
Then you might want to estimate parameters from that simulated data.  Above we created two simulated datasets (`nsim=2`).  We will fit to the first one. Here the default settings for `MARSS()` are used.
```{r sim-fit, include.source=F, keep.source=T, results='hide'}
kem.sim.1 = MARSS(sim.data[,,1])
```
Then we might like to see the likelihood\index{likelihood} of the second set of simulated data under the model fit to the first set of data.  We do that with the Kalman filter function.  This function takes a marssMLE object (as output by say the MARSS function), and we have to replace the data in kem.sim.1 with the second set of simulated data.\index{likelihood!MARSSkf function}
```{r sim-like, include.source=F, keep.source=T}
kem.sim.2 = kem.sim.1
kem.sim.2$model$data = sim.data[,,2] 
MARSSkf( kem.sim.2 )$logLik
``` 

## Bootstrap AIC
The function `MARSSaic()`\index{model selection!MARSSaic function}\index{functions!MARSSaic} computes a bootstrap AIC for model selection purposes.  Use `output="AICbp"` to produce a parameter bootstrap\index{model selection!bootstrap AIC, AICbp}.  Use `output="AICbb"` to produce a non-parametric bootstrap\index{model selection!bootstrap AIC, AICbb} AIC. You will need a large number of bootstraps (`nboot`). We use only 10 bootstraps to show you how to compute AICb with the MARSS package, but the AICbp estimate will be terrible with this few bootstraps.
```{r AICb} 
kemfit.with.AICb = MARSSaic(kemfit, output = "AICbp", 
   Options = list(nboot = 10, silent=TRUE))
#nboot should be more like 1000, but set low here for example sake

print(kemfit.with.AICb)
```


## Convergence
MARSS uses two convergence tests.  The first is
$$\mathrm{logLik}_{i+1}-\mathrm{logLik}_i < \mathrm{tol}$$
This is called `abstol` (meaning absolute tolerance) in the output.  The second is called the `conv.test.slope`. This looks at the slope of the log parameter value (or likelihood) versus log iteration number and asks whether that is close to zero (not changing).  

If you are having trouble getting the model to converge, then start by addressing the following 1) Are you trying to fit a bad model, e.g. a non-stationary model fit to stationary data or the opposite or a model that specifies independence of errors or states to data that clearly violate that or a model that implies a particular stationary distribution (particular mean and variance) to data that strongly violate that? 2) Do you have confounded parameters, e.g. two parameters that have the same effect (like effectively two intercepts)?, 3) Are you trying to fit a model to 1 data point somewhere, e.g. in a big multivariate dataset with lots of missing values?  4) How many parameters are you trying to estimate per data point? 5) Check your residuals (`residuals(kemfit)$model.residuals`) for normality.  6) Did you do any data transformations that would cause one of the variances to go to zero? Replacing 0s with a constant will do that.  Try replacing them with NAs (missing).  Do you have long strings of constant numbers in your data?  Binned data often look like that, and that will drive $\mathbf{Q}$ to 0.
